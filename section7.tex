\section*{Neural Networks}
\textbf{Universal Approximation:} Any cont. $f:\mathbb{R}^n\to\mathbb{R}^p$ can be unif. approx. on comp. supp. $\mathcal{K}$ by NN with 1 hidden layer: $g_k(x)=\sigmoid(\alpha_k+\sum_{j\to k}w_{jk}\sigma(\alpha_j+\sum_{i\to j} w_{ij}x_i))$.\\
\textbf{Robbins-Monro:} If $\E[\gamma]=0$, $\E[\gamma^2]=\sigma^2$, $(\theta-\theta^*)l(\theta) > 0$, $\exists b.\,l<b$, $\sum\eta(t) = \infty$, $\sum\eta(t)^2 < \infty$, then
$\theta^{(t)}\overset{\mathcal{L}^2}{\to}\theta^*$ where
$\theta^{(t+1)} = \theta^{(t)} - \eta(t)(\theta^{(t)}+\gamma^{(t)})$.
\textbf{Representation Learning with InfoMax:} Find $\argmax_\theta I(X;Z) \approx \frac{1}{M}\sum_i\E_{Z|x_i}[\log p(x_i|Z)]$. \emph{Problem: Not robust --- $f_\theta:\mathcal{X}\to\mathcal{Z}$ becomes inj.}\\
\textbf{VAEs:} \emph{Latent vars.} $Z\sim p_\theta(z)$ (Prior)\\
\emph{Encoder} $p_\theta(x|z)$ (Likelihood)\\
\emph{Decoder} $p_\theta(z|x)\approx q_\phi(z|x)$ (Posterior).\\
\emph{\textbf{Training:}} $\max_{\theta,\phi} {\color{gray}\log p_\theta(x_i) \geq}$\\$ \mathcal{L}(x_i,\theta,\phi) = \mathrm{ELBO} = \E_{z\sim q_\phi}[$\\$\log p_\theta(x_i|z)] - D_{\mathrm{KL}}(q_\phi(z|x_i)||p_\theta(z|x_i))$.\\
\textbf{GANs:} Solve $\min_G\max_D\E[\log(D(x))] + \E[\log(1-D(G(Z)))]$. (Latent vars $Z$.)\\
\emph{\textbf{Training:}} Alternate between GD on $\min_G(\dots)$ and GA on $\max_D(\ldots)$.\\
\emph{\textbf{Stability:}} Change GD on $\E[\log 1-D(G(Z))]$ to GA on $\E[\log D(G(Z))]$.

% \subsection*{Boltzmann Machine}
% Symmetric coupling. Visible and Hidden units. Update through voting of neighboors. Find the weights which generate a defined activity of visible nodes.

% \section*{Unsupervised Learning}
% \subsection*{Histograms}
% $p_i = \frac{n_i}{N\Delta_i}$ $n\leq N$ in bin $i$ of size $\Delta_i$\\
% Not scaling to multiple dimensions.
% $K\simeq NP\quad P\simeq p(x)V \Rightarrow p(x) = \frac{K}{NV}$\\
% $K$ \#samples in region of volume $V$, $P$ probability of falling in it. 
% \subsection*{Kernel Density Estimator}
% Fix $V$ and determine K.

% \textbf{Parzen Window}:\\ $K=\sum_{n=1}^N\phi(\frac{x-x_n}{h})$, 
% $ \phi(u)= \mathbb{I}_{\{||x||\leq\frac{1}{2}\}}$\\
% $p(x)=\frac{1}{N}\sum_{n=1}^N\frac{1}{h^D}\phi(\frac{x-x_n}{h})$\\
% This window has discontinuities.\\
% \textbf{Gaussian Kernel}: 
% $ \phi(u){=} \frac{\mathrm{exp}({-}\frac{1}{2}||x||^2)}{\sqrt{2\pi}}$\\
% Result in a smoother density model\\
% $p(x)=\frac{1}{N}\sum_{n=1}^N\frac{1}{(2\pi h^2)^{D/2}}\mathrm{exp}(-\frac{||x-x_n||^2}{2h^2})$
% We can chose any other kernel $\phi$ with $\phi(u)\geq0\quad\int\phi(u)du=1$

% \subsection*{K-Nearest Neighbors}
% Fix $K$ and find $V$\\
% $\hat{p}(x)=\frac{1}{V_k(x)}, v_k(x)$ minimal volume around x containing k neighbors.\\
% \textbf{Classifier}: classify $x$ by the majority of the vote of its k-NN.\\
% \textbf{1-NN Error Rate}
% the 1-NN error rate $P$ is always $P^*\leq P\leq 2P^*$ where $P^*$ is the error rate of the Bayes rule.
% $\Rightarrow$ as k goes to infinity kNN becomes optimal\\
% KNN not optimal if class densities are very different.


\section*{Mixture Models}

\subsection*{Gaussian Mixture}
\textbf{EM-Algorithm}\\
Latent Variable: unknown data $\rightarrow$ What cluster generated each sample?\\
EM does ML for unknown parameters.

Latent var. $M_{\mathbf{x}c}{=}\begin{cases} 
       1 \quad \text{c generated x}\\
       0 \quad \text{else}
       \end{cases} 
$\\
$P(\mathcal{X}, M|\mathbf{\theta}){=}\prod_{x\in\mathcal{X}}\prod_{c=1}^k(\pi_cP(\mathbf{x}|\mathbf{\theta}_c))^{M_{\mathbf{x}c}}$

\textbf{E-Step}\\
$ \gamma_{\mathbf{x}c}{=}\mathbb{E}[M_{\mathbf{x}c}|\mathcal{X},\mathbf{\theta}^{(j)}]{=}\frac{P(\mathbf{x}|c,\mathbf{\theta}^{(j)})P(c|\mathbf{\theta}^{(j)})}{P(\mathbf{x}|\mathbf{\theta}^{(j)})}$\\
\textbf{M-Step}\\
$\mathbf{\mu}_c^{(j+1)}=\frac{\sum_{c\in\mathcal{X}}\gamma_{\mathbf{x}c}\mathbf{x}}{\sum_{c\in\mathcal{X}}\gamma_{\mathbf{x}c}}$\\
$(\sigma_c^2)^{(j+1)}=\frac{\sum_{c\in\mathcal{X}}\gamma_{\mathbf{x}c}(\mathbf{x}-\mathbf{\mu}_c)^2}{\sum_{c\in\mathcal{X}}\gamma_{\mathbf{x}c}}$\\
$\pi_c^{(j+1)}=\frac{1}{|\mathcal{X}|}\sum_{c\in\mathcal{X}}\gamma_{\mathbf{x}c}$\\

% \subsection*{k-Means}
% identify clusters of data.\\
% Given $\mathcal{X}=\{\mathbf{x}_1,\cdots,\mathbf{x}_n\}$\\
% Find $c(.)$ and $\mathcal{Y}$ minimizing\\
% $\mathcal{R}^km(c,\mathcal{Y})=\sum_{x\in\mathcal{X}}||x-\mu_{c(x)}||^2$
% Assign to nearest cluster. Recompute all clusters and repeat. Also called \textbf{hard EM}. Special case of GMM w. uniform prior and diag. covariance ($\rightarrow 0$).