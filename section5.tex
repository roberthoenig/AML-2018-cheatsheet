\section*{Classification}
\definecolor{ForestGreen}{RGB}{34,139,34}

% group points in classes $1,\cdots, k, \mathcal{D}, \mathcal{O}$\\
% $\mathcal{D}$: doubt class, $\mathcal{O}$: outliers.\\
% Data: $\mathcal{Z}=\{z_i=(x_i,y_i):1\leq i\leq n\}$ 
% Assume we know $p_y(x){=}P[X{=}x|Y{=}y]$\\
% Found: classifier $\hat{c}:\mathcal{X}{\rightarrow}\mathcal{Y}{:=}\{1,\cdots, \mathcal{D}\}$\\
% Error: $\hat{R}(\hat{c}|\mathcal{Z})=\sum_{(x_i,y_i)\in\mathcal{Z}}\mathbb{I}_{\{\hat{c}(x_i)\not=y_i\}}$\\
% Expected Error:\\
% $\mathcal{R}(\hat{c}) = \sum_{y\leq k}P[y]\mathbb{E}_{x|y}[\mathbb{I}_{\{\hat{c}(x_i)\not=y_i\}}|Y=y]$
% (add term from $\mathcal{D}$)
% \begin{table}[H]
% \begin{tabular}{l|lll}
%     & Prob. Gen & Prob. Disc & Disc. \\ \hline
% Sample dist. & Yes & No & No \\
% Pred. dist. & Yes & Yes & No \\
% Comp. cost & High & Medium & Low \\
% Bias & High & Medium & Low
% \end{tabular}
% \label{tab:my-table}
% \end{table}

\subsection*{Probabilistic Generative}
\textbf{Steps:} Model $p_\theta(X,Y)$ $\rightarrow$ Compute $\myhat\theta = \mathrm{MLE}(\theta)$ $\rightarrow$ Compute $p_{\myhat\theta}(X|Y)$ $\rightarrow$ Probabilistic Discriminative.\\
\textbf{Example (Mixture of Gaussians):} $p_\theta(X,Y)=Y\beta\mathcal{N}(X;\mu_1,\Sigma_1)+(1-Y)(1-\beta)\mathcal{N}(X;\mu_0,\Sigma_0)$ $\rightarrow$ $\Sigma_1=\Sigma_0 \& \beta=\frac{1}{2} \implies p(Y|X) = \sigma(\vw^TX + w_0)$ (\textbf{LDA}) else $p(Y|X) = \sigma(X^T\mW X + \vw^TX + w_0)$ (\textbf{QDA})
\textbf{{\color{red} High Comp. Cost}, {\color{red}High Bias}}
\subsection*{Probabilistic Discriminative}
\textbf{Steps:} Model $p(Y|X) = Y\sigma(\vw^TX)+(1-Y)(1-\sigma(\vw^TX))$

\textbf{Example (Logistic Regression):} Solve $\frac{\delta p(tr. set)}{\delta\vw}=0$ with gradient descent (or with Newton's method --- \textbf{Iterative Least Squares Regression})

\textbf{{\color{orange} Med. Comp. Cost}, {\color{orange}Med. Bias}}
\subsection*{Discriminative}
\textbf{{\color{ForestGreen} Low Comp. Cost}, {\color{ForestGreen}Low Bias}}


% \subsection*{Loss Functions}
% 0-1 Loss:
% $L^{0-1}(y,z) = \begin{cases} 
%       0 \quad \mathrm{if } (z=y)\\
%       1 \quad \mathrm{if } (z\not=y)\\
%   \end{cases}
% $\\
% Exponential Loss:\\
% $L^{exp}(y,z)=\mathrm{exp}(-(2y-1)(2z-1))$\\
% Logistic Loss:\\
% $L^{log}(y,z)=\mathrm{ln}(1+\mathrm{exp}((2y-1)(2z-1)))$\\
% Hinge Loss:\\
% Favors sparsity. Used in SVM\\
% $L^{hinge}(y,z)=\mathrm{max}\{0,1-(2y-1)(2z-1)\}$


% \subsection*{Bayes Optimal Classifier}
% Minimizes total risk for 0-1 Loss\\
% $\hat{c}(x){=}
% \begin{cases} 
%       y & \mathbb{P}[y|x]>{1-d},\exists y\\
%       \mathcal{D} & \mathbb{P}[y|x]<1-d,\forall y
%   \end{cases}
% $\\
% Generalize to other loss functions

% \subsection*{Discriminant Functions}
% Functions $g_k(x)\quad1\leq k\leq K$\\
% Decide: $g_y(x){>}g_z(x)\forall z \not{=} y {\Rightarrow}$ chose $y$\\
% Const factor doesn't change decision.\\
% $g_k(x)=P[y|x]\propto P[x|y]P[y] \Rightarrow$\\
% $g_k(x){=}lnP[x|y]+lnP[y]{=}lnP[x|y]+\pi_y$
% implements an opt. Baye classifier.

% \subsection*{Decision Surface of Discriminant}
% Solve: $g_{k_1}(x)-g_{k_2}(x)=0$
% Special case with Gaussian classes:\\
% if $\Sigma_y = \Sigma \Rightarrow$ linear decision surface
% $g_k(x){=}w^T(x-x_0)\quad w{=}\Sigma^{-1}(\mu_1{-}\mu_2)$\\
% $x_0{=}\frac{1}{2}(\mu_1{+}\mu_2){-}\frac{\sigma^2(\mu_1-\mu_2)}{(\mu_1-\mu_2)^T\Sigma^{-1}(\mu_1-\mu_2)}\mathrm{log}\frac{\pi_1}{\pi_2}$

% \subsection*{Linear Classifier}
% % optimal for Gaussian with equal cov. Stat. simplicity \& comput. efficiency.
% $g(x)=a^T\tilde{x}\quad a=(w_0,w)^T, \tilde{x}=(1,x)^T$\\
% $a^T\tilde{x}_i>0 \Rightarrow y_i=1, a^T\tilde{x}_i<0 \Rightarrow y_i=2$\\
% Normalization: $\tilde{x}_i\rightarrow-\tilde{x}_i$ if $y_i=2$
% Find $a$: $a^T\tilde{x}_i>0,\forall_i$!\\
% Learning w. Gradient Descent:\\
% $a(k+1)=a(k)-\eta(k)\nabla J(a(k))$\\
% $J(.)$: cost function $\eta(.)$: learning rate\\
% Newton's rule (opt. grad descent):\\
% $a(k+1)=a(k)-H^{-1}\nabla J, H=\frac{\partial^2 J}{\partial a_i \partial a_j}$\\
% Taylor: $f(x)=f(a)+(x-a)f'(a)+\dots$
\textbf{Example (Perceptron):}\\
$c(x) = \mathrm{sign}(\vw^Tx)$\\
$\mathcal{L}(\vw)=\sum_{\tilde{x}\in\mathcal{X}^\text{msc}}-w^T\tilde{x}$,\\
$\frac{\delta\mathcal{L}(\vw)}{\delta\vw} = -\sum_{x_i\in\mathcal{X}^\text{msc}}y_ix_i$\\
\textbf{Variable increment perceptron:}\\
\mbox{\hspace{-0.5em}$\vw^{(k+1)}=\vw^{(k)}+\eta^{(k)}y_kx_k $ if $y_k{w^{(k)}}^Tx < 0$}\\
Data linearly separable \& $\eta$ satisfies Robbins-Monro $\implies$ CV. 
\subsection*{Fisher's Linear Discriminant Analysis}
$\implies$ A Linear Classifier projects training set onto
single dimension.\\
$\implies$ Max. between-class distance, min. within-class distance.\\
\textbf{Objective:} \mbox{$\max_\vw J(\vw) = \frac{(\vw^T\bar{x_0} - \vw^T\bar{x_1})^2}{D_0+D_1}$}
where $D_i = \sum_{x\in C_i}{(\vw^Tx - \vw^Tx_i)}$\\
\textbf{Solution:}
$\vw^* \propto S^{-1}(\bar{x_1}-\bar{x_0})$ where $S = \sum_{c\in\{0,1\}}\sum_{x\in C_c}(x-\bar{x_c})(x-\bar{x_c})^T$

\subsection*{Lagrangian Optimization}
\textbf{Primal:} Find $\argmin_\vw f(\vw)\\s.t. \forall i,j.\,\,\,g_i(\vw)=0\,\,\,\&\,\,\,h_j(\vw) \leq 0$ for convex $g_i, h_j$.\\
\textbf{Dual:} Find $\max_{\alpha\geq0,\lambda} {\theta(\lambda, \alpha)}$ with ${\theta(\lambda, \alpha)} = \min_\vw\mathcal{L}(\vw, \lambda, \alpha)$.\\
\textbf{Lagrangian:} $\mathcal{L}(\vw, \mathbf{\lambda}, \mathbf{\alpha}) = f(\vw) + \sum\lambda_i g_i(\vw) + \sum\lambda_j h_j(\vw)$.\\
$\mathcal{L}(\vw, \mathbf{\lambda}, \mathbf{\alpha})=0 \implies \vx^*$ solves Primal.\\
If $f, g_i, h_i$ convex \& \textbf{Slater's condition} holds \mbox{($\exists \vw. \forall i. g_i(\vw)=0\,\,\&\,\,h_j\mathbf{<}0$)} then \textbf{Strong duality} ($\vw^* = \argmin_\vw \mathcal{L}(\vw, \lambda^*, \alpha^*)$) and \textbf{Complementary slackness} ($\alpha^*_jh^*_j(\vw^*) = 0$)


\subsection*{Support Vector Machine (SVM)}
Generalize Perceptron with margin and kernel:
Find $\max_\vw \mathrm{margin}(\vw) = \max_\vw\min_i y_i(\frac{\vw^T}{||\vw||}\vx_i + w_0)$. Equivalent to \textbf{Primal}: $\min_\vw \frac{1}{2}||\vw||^2\,\,\mathrm{s.t.}\,\, y_i(\vw^Tx_i+w_0) \geq 1$. Get \textbf{Lagrangian} $\mathcal{L}(\vw, \lambda, \alpha) = \frac{1}{2}||\vw||^2 + \sum\alpha_j (1-y_j(\vw^T\vx_j+w_0))$. Solve $\nabla\mathcal{L}=0, \frac{d\mathcal{L}}{dw_0}=0$ to get \textbf{Dual} $\max_\alpha -\frac{1}{2}\sum a_ia_jy_iy_j\vx^T_i\vx_j + \sum\alpha_i$ s.t.
$\alpha_j\geq 0$ \& $\sum a_jy_i = 0$. Solve with \textbf{quadratic optimization}. Optimal $\vw^* = \sum_i \alpha^*_i y_i x_i, w_0^* = -\frac{1}{2}({\vw^*}^T{\vx^+}^T + {\vw^*}^T{\vx^-}^T)$. Complementary slackness $\color{black}\implies$ $\alpha$ sparse.
\subsection*{Soft Margin SVM}
Introduce slack to relax constraints.
\textbf{Primal:} $\min_\vw \frac{1}{2}||\vw||^2 + C\sum\xi_i$ s.t. $y_i(\vw^Tx_i+w_0) \geq 1-\xi_i$ and $\xi_i \geq 0$.
\textbf{Changes in Dual:} $0\leq\alpha_i{\color{red}\leq C}$.
Large $C$ $\color{black}\implies$ narrow margin, little slack. 

\subsection*{Non-Linear SVM}
Consider the feature map $\vx \rightarrow \phi(\vx)$ induced by kernel $K(\vx, \vx')$. The \textbf{Dual} then becomes $\max_\alpha -\frac{1}{2}\sum a_ia_jy_iy_j{\color{red}K(\vx_i, \vx_j)} + \sum\alpha_i$ s.t.
$\alpha_j\geq 0$. For classification, we get ${\vw^*}^T\vx + w_0 = \sum_i \alpha_i^*y_iK(\vx_i, \vx) + w_0$.

\subsection*{Multiclass SVM}
$\forall$class $z\in\{1,2,\cdots,M\}$ we introduce $\mathbf{w}_z$ and define the margin $m$ s.t.:\\
$(\mathbf{w}_{z_i}^T\mathbf{y}_i+w_{z_i,0})-\max_{z\not=Z_i}(\mathbf{w}_z^T\mathbf{y}_i+w_{z,0})\geq m, \forall_{\mathbf{y}_i\in \mathcal{Y}}$

\subsection*{Structured SVM}
Each sample \textbf{y} is assigned to a structured output label $z$\\
Output Space Representation:\\
joint feature map: $\mathbf{\psi}(z,\mathbf{y})$\\
Scoring function: $f_{\mathbf{w}}(z,\mathbf{y})=\mathbf{w}^T\mathbf{\psi(z, \mathbf{y})}$\\
Classify: $\hat{z}=h(\mathbf{y})\argmax_{z\in\mathcal{K}}f_{\mathbf{w}(z, \mathbf{y})}$


\subsection*{Kernels}
Similarity based reasoning\\
Gram Matrix $K{=}K(\mathbf{x}_i, \mathbf{x}_i), 1{\leq} i,j{\leq} n$\\
$K(\mathbf{x}, \mathbf{x'}) {=} \phi(\mathbf{x})^T\phi(\mathbf{x'})\quad K(\mathbf{x},\mathbf{x'}){=}K(\mathbf{x'},\mathbf{x})$\\
$K(\mathbf{x},\mathbf{x'})$ pos.semi-def. (all EV $\geq$ 0)\\
If $K_1$ \& $K_2$ are kernels $K$ is too:\\
$K(\mathbf{x}, \mathbf{x'})=K_1(\mathbf{x}, \mathbf{x'})K_2(\mathbf{x}, \mathbf{x'})$\\
$K(\mathbf{x},\mathbf{x'})=\alpha K_1(\mathbf{x}, \mathbf{x'})+\beta K_2(\mathbf{x}, \mathbf{x'})$\\
$K(\mathbf{x},\mathbf{x'}){=}K_1(h(\mathbf{x}), h(\mathbf{x'}))\quad h:\mathcal{X}{\rightarrow}\mathcal{X}$\\
$K(\mathbf{x},\mathbf{x'}){=}h(K_1(\mathbf{x}, \mathbf{x'}))\quad h$: poly/exp\\
Kernel Function Examples:\\
$K(\mathbf{x},\mathbf{x'}){=}\mathbf{x}^T\mathbf{x'}\quad K(\mathbf{x},\mathbf{x'}){=}(\mathbf{x}^T\mathbf{x'}{+}1)^p$\\
RBF(Gauss):$K(\mathbf{x},\mathbf{x'}){=}\mathrm{exp}(-||\mathbf{x}{-}\mathbf{x'}||_2^2/h^2)$\\
Sigmoid:$K(\mathbf{x},\mathbf{x'}){=}\mathrm{tanh}(\alpha\mathbf{x}^T\mathbf{x'}+c)$\\
not p.s-d eg: $\mathbf{x}{=}[1,-1], \mathbf{x'}{=}[-1,2]$