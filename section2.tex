\section*{Regression}
\textbf{Data}:
$D=\{(x_i,y_i)\}_{i=1}^n$, i.i.d. $\sim P(X,Y)$\\
\textbf{Risk}:
$\mathbf{E}[(Y - f(X))^2]$\\
\textbf{Optimal solution:} $f(x) = E[Y|X=x]$\\
\textbf{Lin. regr:}
$\mathbf{Y}=X^T\beta + \epsilon$, $\epsilon \sim \mathcal{N}(0,\sigma^2\mathbb{I})$\\
\textbf{SVD:} $\mathbf{X} = \mU\mD\mV^T$


\subsection*{(1) Ordinary Least Squares}
\textbf{MLE, ERM:} $\min_{\myhat{\beta}}(\mathbf{y}-\mathbf{X}\myhat{\beta})^T(\mathbf{y}-\mathbf{X}\myhat{\beta})$\\
% $X\in\mathbb{R}^{n\times(d+1)}, y\in\mathbb{R}^n,  \beta\in\mathbb{R}^{d+1}$\\
\textbf{Solution:} $\hat{\beta}^\text{OLS} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}$\\
$\hat{\beta}^\textrm{OLS} \sim \mathcal{N}(\beta, (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2)$\\
\textbf{SVD:} $\hat{\mathbf{y}} = \mathbf{X}\hat{\beta}^\text{ridge} = \mU\mU^T\mathbf{y}$

\subsection*{(2) Ridge Regression ($L^2$ penalty)}
\textbf{Prior:} $\beta \sim \mathcal{N}(0, \frac{\sigma^2}{\lambda}\mathbb{I})$\\
\textbf{MAP:} $\min_{\myhat{\beta}}(\mathbf{y}-\mathbf{X}\myhat{\beta})^T(\mathbf{y}-\mathbf{X}\myhat{\beta})+\lambda\myhat{\beta}^T\myhat\beta$\\
\textbf{Solution:} $\hat{\beta}^\text{ridge} = (\mathbf{X}^T\mathbf{X}+\lambda\mathbb{I})^{-1}\mathbf{X}^{T}\mathbf{y}$
\textbf{SVD:} $\hat{\mathbf{y}} = \mathbf{X}\hat{\beta}^\text{ridge} = \sum_{i=1}^d{\vu_i\frac{d_i^2}{d_i^2+\lambda}\vu_i^T\mathbf{y}}$

\subsection*{(3) Lasso ($L^1$ penalty, sparse)}
\textbf{Prior:} $p(\beta_i) = \frac{\lambda}{4\sigma^2}\mathrm{exp}(-|\beta_i|\frac{\lambda}{2\sigma^2})$\\
\textbf{MAP:} $\min_{\myhat{\beta}}(\mathbf{y}-\mathbf{X}\myhat{\beta})^T(\mathbf{y}-\mathbf{X}\myhat{\beta})+\lambda||\myhat\beta||_1$\\
\textbf{Solution} Efficient opt. techniques.\\
\textbf{Alternative objective:} $\min_{\myhat{\beta}}(\mathbf{y}-\mathbf{X}\myhat{\beta})^T(\mathbf{y}-\mathbf{X}\myhat{\beta})\,\,\, \mathrm{s.t.}\,\,\, ||\myhat\beta||_1 \leq s$.

\textbf{Note:} Norm $L^a, a<1$ is non-convex, harder to optimize.

\subsection*{(4) Bayesian Linear Regression}
\textbf{Setting:} Define a prior over $\beta$.\\
\textbf{e.g. Ridge:} Assume $\beta$ distributed as:\\
$p(\beta|\bm{\bm{\Lambda}}){=}\mathcal{N}(\beta|\mathbf{0},\frac{\sigma^2}{\lambda}\mathbb{I}) \propto \mathrm{exp}(-\frac{\lambda}{2\sigma^2}\beta^T\beta)$\\
For $\bm{\Lambda}=\frac{\sigma^2}{\lambda}\mathbb{I}$. Linear for $\sigma=1$.

Then, given observed $\mathbf{X},\mathbf{y}$, use Bayes' theorem to find the posterior\\
$p(\beta|\mathbf{X},\mathbf{y}, \bm{\Lambda}, \sigma) = \mathcal{N}(\mu_{\beta}, \Sigma_{\beta})$\\
$\mu_\beta = \sigma^2(\mathbf{X}^T\mathbf{X} +\sigma^2\bm{\Lambda})^{-1}\mathbf{X}^T\mathbf{y}$\\
$\Sigma_\beta = \sigma^2(\mathbf{X}^T\mathbf{X} +\sigma^2\bm{\Lambda})^{-1}$

\subsection*{(4)Basis expansion}
\textbf{Idea:} Feature space transformation\\
Model: $\mathbf{Y}=f(\mathbf{X})=\sum_{m=1}^M\beta_m h_m(\mathbf{X})$\\
Transformation $h_m(\mathbf{X}):\mathbb{R}^d \rightarrow \mathbb{R}$

\subsection*{(5) Gaussian Processes}
joint Gaussian over all outputs\\
$\mathbf{y}=\mathbb{X}\beta+\epsilon \quad \epsilon\sim \mathcal{N}(\epsilon|0,\sigma\mathbb{I}_n)$\\
We can rewrite the distribution\\
$P(\begin{bmatrix}
\mathbf{y}\\
y_*\\
\end{bmatrix}){=}\mathcal{N}(\mathbf{y}|\mathbf{0},\begin{bmatrix}
\mathbf{C_n} & \mathbf{k} \\
\mathbf{k^T} & c \\
\end{bmatrix})$\\
Such that for \textbf{prediction}:\\
$p(y_*|\mathbf{x_*}, \mathbf{X}, \mathbf{y}){=} \mathcal{N}(y_*|\mu_{*}, \sigma^2_{*})$\\
$\mu_{y_*} = \mathbf{k}^T\mathbf{C}_n^{-1}\mathbf{y}\quad\ \  \mathbf{C}_n=\mathbf{K}+\sigma^2\mathbb{I}$\\
$\sigma^2_{*}{=}c{-}\mathbf{k}^T\mathbf{C}_n^{-1}\mathbf{k}\quad c{=}k(x_*,x_*){+}\sigma^2$\\
$\mathbf{k}=k(x_*,\mathbf{X})\quad\ \ \ \ \ \ \mathbf{K}_{ij}=k(x_i,x_j)$\\
$k$ is the kernel function. Lengthscale: how far can we reliably extrapolate.


\subsection*{Gauss-Markov Theorem}
For any unbiased linear estimator $\tilde{\theta} = c^T\mathbf{y}$ of $a^T\mathbf{\beta}$, it holds that $\Var(a^T\myhat{\beta}^{\mathrm{OLS}}) \leq \Var(c^T\mathbf{y})$.

\subsection*{Bias-Variance tradeoff}
$\color{gray}\mathbb{E}[\hat{f_D}(x)-Y)^2|X=x]=$\\
$\mathbb{E}_D\mathbb{E}_{Y|X=x}[(\hat{f_D}(x)-Y)^2]=$\\
$\color{gray}\mathbb{E}_D[(\hat{f_D}(X)-\mathbb{E}[Y|X=x])^2]+$\\$\color{gray}\mathbb{E}[Y-\mathbb{E}[Y|X=x]]^2=$\\
$\mathbb{E}_D[(\hat{f}_D(x)-\mathbb{E}_D[\hat{f}(x)])^2]$ (var.)\\
$+(\mathbb{E}_D[\hat{f}_D(x)]-\mathbb{E}_{Y|X=x}[Y])^2$ (bias$^2$)\\
$+\mathbb{E}[Y-\mathbb{E}[Y|X=x]]^2$ (noise$^2$)\\

$|\mathcal{X}|\downarrow \quad|\mathcal{C}|\uparrow\quad\Rightarrow\quad\mathrm{var.}\uparrow\quad\mathrm{bias^2}\downarrow $\\
$|\mathcal{X}|\uparrow \quad|\mathcal{C}|\downarrow\quad\Rightarrow\quad\mathrm{var.}\downarrow\quad\mathrm{bias^2}\uparrow $
