\section*{Regression}
\textbf{Data}:
$D=\{(x_i,y_i)\}_{i=1}^n$, iid. $\sim P(X,Y)$\\
\textbf{Risk}:
$\mathbf{E}[(Y - f(X))^2]$\\
\textbf{Optim. solution:} $f(x) = E[Y|X=x]$\\
\textbf{Lin. regr:}
$\mathbf{Y}=X\beta + \epsilon$, $\epsilon \sim \mathcal{N}(0,\sigma^2\mathbb{I})$\\
\textbf{SVD:} $\mathbf{X} = \mU\mD\mV^T$


\subsection*{(1) Linear Regression}
\textbf{MLE, ERM:} $\min_{\myhat{\beta}}(\mathbf{y}-\mathbf{X}\myhat{\beta})^T(\mathbf{y}-\mathbf{X}\myhat{\beta})$\\
% $X\in\mathbb{R}^{n\times(d+1)}, y\in\mathbb{R}^n,  \beta\in\mathbb{R}^{d+1}$\\
\textbf{Solution:} $\hat{\beta}^\text{OLS} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}$\\
$\hat{\beta}^\textrm{OLS} \sim \mathcal{N}(\beta, (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2)$\\
\textbf{SVD:} $\hat{\mathbf{y}} = \mathbf{X}\hat{\beta}^\text{ridge} = \mU\mU^T\mathbf{y}$

\subsection*{(2) Ridge Regression ($L^2$ penalty)}
\textbf{Prior:} $\beta \sim \mathcal{N}(0, \frac{\sigma^2}{\lambda}\mathbb{I})$\\
\textbf{MAP:} $\min_{\myhat{\beta}}(\mathbf{y}-\mathbf{X}\myhat{\beta})^T(\mathbf{y}-\mathbf{X}\myhat{\beta})+\lambda\myhat{\beta}^T\myhat\beta$\\
\textbf{Solution:} $\hat{\beta}^\text{ridge} = (\mathbf{X}^T\mathbf{X}+\lambda\mathbb{I})^{-1}\mathbf{X}^{T}\mathbf{y}$
\textbf{SVD:} $\hat{\mathbf{y}} = \mathbf{X}\hat{\beta}^\text{ridge} = \sum_{i=1}^d{\vu_i\frac{d_i^2}{d_i^2+\lambda}\vu_i^T\mathbf{y}}$

\subsection*{(3) Lasso ($L^1$ penalty, sparse)}
\textbf{Prior:} $p(\beta_i) = \frac{\lambda}{4\sigma^2}\mathrm{exp}(-|\beta_i|\frac{\lambda}{2\sigma^2})$\\
\textbf{MAP:} $\min_{\myhat{\beta}}(\mathbf{y}-\mathbf{X}\myhat{\beta})^T(\mathbf{y}-\mathbf{X}\myhat{\beta})+\lambda||\myhat\beta||_1$\\
\textbf{Solution} Efficient opt. techniques.\\
\textbf{Alternative objective:} $\min_{\myhat{\beta}}(\mathbf{y}-\mathbf{X}\myhat{\beta})^T(\mathbf{y}-\mathbf{X}\myhat{\beta})\,\,\, \mathrm{s.t.}\,\,\, ||\myhat\beta||_1 \leq s$.

\textbf{Note:} Norm $L^a, a<1$ is non-convex, harder to optimize.

\subsection*{(4) Bayesian Linear Regression}
\textbf{Prior:} $\beta\sim\mathcal{N}(\mathbf{0},\Lambda^{-1})$\\
\textbf{Posterior:} $p(\beta;\mathbf{X},\mathbf{y}) = \mathcal{N}(\beta; \mu_{\beta}, \Sigma_{\beta})$\\
$\mu_\beta = (\mathbf{X}^T\mathbf{X} +\sigma^2\bm{\Lambda})^{-1}\mathbf{X}^T\mathbf{y}$\\
$\Sigma_\beta = \sigma^2(\mathbf{X}^T\mathbf{X} +\sigma^2\bm{\Lambda})^{-1}$\\
\textbf{Predictions:}
$y^* = {x^*}^T\beta + \epsilon$\\$\sim \mathcal{N}({x^*}^T\mu_{\beta}, {x^*}^T\Sigma_{\beta}{x^*}+\sigma^2)$
\subsection*{(4)Basis expansion}
\textbf{Idea:} Feature space transformation\\
Model: $\mathbf{Y}=f(\mathbf{X})=\sum_{m=1}^M\beta_m h_m(\mathbf{X})$\\
Transformation $h_m(\mathbf{X}):\mathbb{R}^d \rightarrow \mathbb{R}$

\subsection*{(5) Gaussian Processes}
\textbf{Definition:} $f: \mathcal{X} \rightarrow \mathbb{R} \sim \mathrm{GP}(\mu, k)$ iff $\forall A \subset \mathcal{X}\!.\, f_A \sim \mathcal{N}(\mu_A, \mK_{AA})$\\
\textbf{Model:} 
$\rvy = f_A + \mathbf{\epsilon}$, $\mathbf{\epsilon} \sim \mathcal{N}(0, \sigma^2\mathbb{I})$\\
\textbf{Prediction:}\\
$p(f;A,\rvy) \sim \mathrm{GP}(f;\mu', k')$ with\\
\mbox{\hspace{-1em}$\mu'(\vx) = \mu(\vx) + \vk_{\vx,A}(\mK_{AA} + \sigma^2\mathbb{I})^{-1}(\rvy_A - \mu_A)$}\\
\mbox{\hspace{-2em}
$k'(\vx,\vx') = k(\vx,\vx') - \vk_{\vx,A}(\mK_{AA}+\sigma^2\mathbb{I})^{-1}\vk^T_{\vx',A}$}\\
% \textbf{Derivation:}\\
% $p(\begin{bmatrix}
% \mathbf{y}\\
% y_*\\
% \end{bmatrix}){=}\mathcal{N}(\begin{bmatrix}
%     \mathbf{y}\\
%     y_*\\
%     \end{bmatrix};\begin{bmatrix}
%         \mathbf{\mu}_A\\
%         \mu_\vx\\
%         \end{bmatrix},\begin{bmatrix}
% \mathbf{C} & \mathbf{k} \\
% \mathbf{k^T} & c \\
% \end{bmatrix})$\\
% The \textbf{conditional Gaussian} formula then gives:
% $p(y_*|\mathbf{x_*}, A, \mathbf{y}){=} \mathcal{N}(y_*;\mu_{*}, \sigma^2_{*})$\\
% $\mu_{*} = \mu_\vx + \mathbf{k}_{\vx,A}^T\mathbf{C}^{-1}(\mathbf{y}-\mu_A)$\\
% $\sigma^2_{*}{=}c{-}\mathbf{k}_{\vx,A}^T\mathbf{C}^{-1}\mathbf{k}_{\vx,A}$\\
% $\mathbf{C}=\mathbf{K}_{AA}+\sigma^2\mathbb{I} \quad c{=}k(x_*,x_*){+}\sigma^2$

\subsection*{Gauss-Markov Theorem}
For any unbiased linear estimator $\tilde{\theta} = c^T\mathbf{y}$ of $a^T\mathbf{\beta}$, it holds that $\Var(a^T\myhat{\beta}^{\mathrm{OLS}}) \leq \Var(c^T\mathbf{y})$.

\subsection*{Bias-Variance tradeoff}
$\color{gray}\mathbb{E}[\hat{f_D}(x)-Y)^2|X=x]=$\\
$\mathbb{E}_D\mathbb{E}[(\hat{f_D}(x)-Y)^2|X=x]=$\\
$\color{gray}\mathbb{E}_D[(\hat{f_D}(X)-\mathbb{E}[Y|X=x])^2]+$\\$\color{gray}\mathbb{E}[Y-\mathbb{E}[Y|X=x]]^2=$\\
$\mathbb{E}_D[(\hat{f}_D(x)-\mathbb{E}_D[\hat{f}(x)])^2]$ (variance)\\
$+(\mathbb{E}_D[\hat{f}_D(x)]-\mathbb{E}[Y|X=x])^2$ (bias$^2$)\\
$+\mathbb{E}[Y-\mathbb{E}[Y|X=x]]^2$ (noise$^2$)\\

$|\mathcal{X}|\downarrow \quad|\mathcal{C}|\uparrow\quad\Rightarrow\quad\mathrm{var.}\uparrow\quad\mathrm{bias^2}\downarrow $\\
$|\mathcal{X}|\uparrow \quad|\mathcal{C}|\downarrow\quad\Rightarrow\quad\mathrm{var.}\downarrow\quad\mathrm{bias^2}\uparrow $

\textbf{Bayesian Information Criterion}\\
$\mathbf{BIC}=-\log\mathbb{P}(\mX, \rvy|\vw^*)+\frac{d}{2}\log(N)$ where $N=\mathrm{len}(\mX)$, $d=\mathrm{len}(\vw^*)$, $\vw^*=MLE(\vw)$. Prefer smaller BIC. Tendency to underfit. Min. BIC $\implies$ approx. max. $\mathbb{P}(\mX, \rvy)$.\\