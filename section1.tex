\subsection*{Distributions}
$\normal(x;\mu, \sigma^2)=\sqrt{2\pi\sigma^2}e^{-(x-\mu)^2/(2\sigma^2)}$\\
$\normal(x;\bm{\mu}, \bm{\Sigma})= |2\pi\bm{\Sigma}|^{-1/2} e^{-\frac{1}{2}(\mathbf{x}-\bm{\mu})^T\bm{\Sigma}^{-1}(\mathbf{x}-\bm{\mu})} $\\
$\mathrm{Exp}(x|\lambda){=}\lambda e^{-\lambda x}$, $\mathrm{Ber}(x|\theta){=}\theta^x (1{-}\theta)^{(1-x)}$
$\mathrm{Dir}(\pi|\alpha) = \frac{1}{B(\alpha)}\prod_{i\leq n}\pi_i^{\alpha_i-1}$.
\subsection*{Conditional Gaussians}
\textbf{Given:} $X_V \sim N(\mu_V, \Sigma_{VV})$, $V=A\cupdot B$\\
\textbf{Then:} $P(X_A|X_B=x_B)=N(\mu_{A|B}, \Sigma_{A|B})$\\
$\mu_{A|B}=\mu_A+\Sigma_{AB}\Sigma^{-1}_{BB}(x_B-\mu_B)$\\
$\Sigma_{A|B}=\Sigma_{AA}-\Sigma_{AB}\Sigma^{-1}_{BB}\Sigma_{BA}$

\subsection*{Functions}
$\sigma(x)=\frac{1}{1+e^{-x}}$, $\sigma'(x)=\sigma(x)(1-\sigma(x))$

\subsection*{Modes of Convergence}
$X_n \overset{p}{\to} X \coloneqq P(|X_n-X|>\epsilon){\to} 0 \,\,\,\,\forall \epsilon > 0$.

\subsection*{Robbins-Monro Conditions}
(1) $\sum\eta(t) = \infty$ and (2) $\sum\eta(t)^2 < \infty$

\subsection*{Taylor expansion {\color{gray} around $\color{gray}\hat\vw$}}
$f(\vw) \approx f(\hat\vw) + (\vw - \hat\vw)^T \nabla f(\hat\vw) + \frac{1}{2}(\vw-\hat\vw)^TH_f(\hat\vw)(\vw-\hat\vw)$

\subsection*{Laplace's method}
$p(\rvw) \propto \exp(-R(\rvw))$ where $R(\rvw) = -\log(p(\rvw))$\\ $\approx R(\rvw^*) - \frac{1}{2}(\rvw-\rvw^*)^TH_R(\rvw^*)(\rvw-\rvw^*)$\\ with $\rvw^* = \argmin_\rvw R(\rvw)$. Thus, $p(\rvw) \approx \mathcal{N}(\rvw;\rvw^*, H_R^{-1}(\rvw^*))$.

\subsection*{Chebyshev's inequality}
$\mathbb{P}(|X-\mathbb{E}[X]|\geq \epsilon)\leq \frac{\mathbb{V}[X]}{\epsilon^2}$\\

\subsection*{Hoeffding's inequality}
For ind. $(X_i)$ with $a_i\leq X_i\leq b_i$, $S_n=\sum_{i\leq n}X_i$, we have $P(S_n-\E S_n\geq t)$,l $P(\E S_n - S_n \geq t)$ $\leq \exp(-\frac{2t^2}{\sum(b_i-a_i)^2})$.\\

\textbf{Other inequalities:} $1-x \leq e^{-x}$\quad

\subsection*{Matrix Calculus}
$\frac{\partial \mathbf{a}^T\mathbf{x}}{\partial\mathbf{x}}{=}\mathbf{a} \quad \frac{\partial \mathbf{a}^T\mathbf{Xb}}{\partial\mathbf{X}}{=}\mathbf{ab}^T \quad \frac{\partial \mathbf{a}^T\mathbf{X}^T\mathbf{b}}{\partial\mathbf{X}}{=}\mathbf{ba}^T $\\
$\frac{\partial \mathbf{a}^T\mathbf{Xa}}{\partial\mathbf{a}}{=}\mathbf{a}^T(\mathbf{X}+\mathbf{X}^T)$,$\frac{\partial \mathbf{K}^{-1}}{\partial K}=-\mathbf{K}^{-1}\mathbf{K}'\mathbf{K}^{-1}$\\
 $\frac{\partial}{\partial\mathbf{x}} \mathbf{f(x)}^T\mathbf{g(x)}{=}\mathbf{f(x)}^T\frac{\partial \mathbf{g(x)}}{\partial\mathbf{x}}+\mathbf{g(x)}^T\frac{\partial\mathbf{f(x)}}{\partial\mathbf{x}}$\\
$\mathbf{X}^T\mathbf{X}$: invertible if no no zero eigenvalues.
Inversion unstable if ratio from $\mathbf{X}$'s smallest EV to the largest is big.

\section*{Representations}
\subsection*{Taxonomy of data}
\textbf{Object space} $\mathcal{O}$. \textbf{Data space} $\mathcal{K}$.

\textbf{Measurement} $X:\mathcal{O}^{(1)}\times\cdots\times\mathcal{O}^{(R)} \rightarrow \mathcal{K}$.

\textbf{Features} derived from msrmnts.

\textbf{Monadic}: $\mathcal{O}^{(1)} \rightarrow \mathbb{R}^D$ (vectorial)

\textbf{Dyadic}: $\mathcal{O}^{(1)}\times\mathcal{O}^{(2)} \rightarrow \mathbb{R}$ (similarity)

\textbf{Polyadic}: $\mathcal{O}^{(1)}\times\mathcal{O}^{(2)}\times\mathcal{O}^{(3)} \rightarrow \{-1, 0, 1\}$ (preference)

\subsection*{Scales}

Characterized by invariance transformation $f: \mathbb{R} \rightarrow \mathbb{R}$

\textbf{Nominal}: categorical, $f \textrm{bijective}$.

\textbf{Ordinal}: ranking, $f \textrm{monotone}$.

\textbf{Interval}: differences, $f(x) = ax + b$.

\textbf{Ratio}: diff \& meaningful 0, $f(x) = ax$.

\textbf{Absolute}: all values meaningful, $f(x) = x$. 

\section*{Risk}
Conditional Expected Risk (RV)\\
$R(f, X) = \E_Y[\mathcal{L}(Y,f(X))|X] = \int \mathcal{L}(Y,f(X))\mathbb{P}(Y|X)\di Y$\\
Total Expected Risk\\
$R(f) = \mathbbm{E}_{X}[R(f,X)] = \E[\mathcal{L}(Y, f(X))]$.

% \subsection*{Empirical Risk}
% Training error:\\
Data $Z^\text{train}={(X_1,Y_1),...,(X_n,Y_n)}$ \\
Empirical Risk (Training error):\\
$\hat{R}(\hat{f}, Z^{train}) = \frac{1}{n} \sum_{i=1}^n \mathcal{L}(Y_i, \hat{f}(X_i))$\\
Empirical Risk Minimizer (ERM) $\hat{f}$:\\
$\hat{f} \in \argmin_{f \in \mathcal{C}} \hat{R}(\hat{f}, Z^{train})$\\

\section*{Estimation theory}

\subsection*{Maximum Likelihood (MLE)}
Find  max. likelihood parameters $\myhat{\theta}$:
$\hat{\theta}\in \argmax_\theta \mathbb{P}(\mathcal{X}|\theta) =\prod_{i\leq n}p(x_i|\theta)$\\
Consistent, asymptotically normal, asymptotically efficient.

\subsection*{Maximum A Posteriori (MAP)}
Given prior $\mathbb{P}(\theta)$, likelihood $\mathbb{P}(\mathcal{X}|\theta)$:\\  MAP estimate $\hat{\theta}\in \argmax_\theta P(\theta|\mathcal{X})$\\ $=\argmax_\theta P(\mathcal{X}|\theta)P(\theta)$\\

\subsection*{Fisher information}
\textbf{Score} $\Lambda = \frac{\partial}{\partial\theta} \log p(X;\theta)$ | $\E[\Lambda] = 0$\\
\textbf{Fisher information} $\mathcal{I}(\theta) = \Var(\Lambda) = \E[\Lambda^2] \overset{(*)}{=}
-\E[\frac{\partial}{\partial\theta^2} \log p(X;\theta)] = \mathcal{J}(\theta)$
Generalizes naturally to random vector $\mathbb{\theta}$ using $\Cov$ and $\frac{\partial}{\partial\theta\partial\theta^T}$.

\subsection*{Estimator properties}

Given estimand $\theta_0$, estimator $\myhat{\theta}_n$.

\textbf{Consistency}: $\myhat{\theta}_n \overset{p}{\to} \theta_0$\\
\textbf{Asymp. normality}: $\sqrt{n}(\myhat{\theta}_n-\theta_0) \overset{d}{\to} \mathcal{N}(0, J^{-1}(\theta_0)I(\theta_0)J^{-1}(\theta_0))$\\
\textbf{Asymp. efficiency}:
$\E[(\myhat{\theta}_n - \theta_0)^2]\sim 1/I_n(\theta_0)$.

\textbf{Equivariance}:
$\myhat{\theta}_n = \textrm{MLE}(\theta_0) \implies g(\myhat{\theta}_n) = \textrm{MLE}(g(\theta_0))$


\subsection*{Stein estimator}
Given $\mathbf{x} \sim \mathcal{N}(\theta_0, \sigma^2I_d)$, $d\geq3$:\\
Stein estimator: $\myhat{\theta}_{JS} = (1-\frac{(d-2)\sigma^2}{||\mathbf{x}||^2})\mathbf{x}$
MLE $\myhat{\theta} = \mathbf{x}$ has $\E[(\myhat{\theta}_{JS} - \theta_0)^2] \leq\!\!/\!\!< \E[(\myhat{\theta} - \theta_0)^2]$ for all / for some $\theta_0$.

% 
% 
\subsection*{Rao-Cramer lower bound}    
$\E[(\myhat{\theta} - \theta_0)^2] \geq (\frac{\partial}{\partial\theta}(b_{\myhat{\theta}}+1)^2)\mathcal{I}(\theta)^{-1} + b_{\myhat{\theta}}^2$\\
Derivation: 
$\frac{\partial}{\partial\theta}(b_{\myhat{\theta}}+1)^2 = 
 \E[\Lambda\myhat{\theta}]^2 = \E[\Lambda(\myhat{\theta} - \E[\myhat{\theta}])]^2 \leq \E[\Lambda^2]\E[(\myhat{\theta}-\E[\myhat{\theta}])^2] = \mathcal{I}(\theta)\E[(\myhat{\theta} - \theta_0)^2] + b_{\myhat{\theta}}^2
$

\subsection*{High-dimensional MLE}
MLE estimate biased if no. of samples grows with no. of parameters:
$\frac{1}{d}\sum_{j\leq d}(\hat w_j-\alpha_* w_j)\overset{a.s.}{\underset{d,N\to\infty}{\to}}0$, $a^* \neq 1$.

\section*{Optimization}
\subsection*{Gradient Descent}
$\vw^{(k+1)} \leftarrow \vw^{(k)} - \eta(k)\nabla\mathrm{NLL}(\vw^{(k)})$ {\color{gray}  $\color{gray}+ \mu(k)(\vw^{(k+1)} - \vw^{(k)})$}(momentum to reduce zigzag)\\
\textbf{Optimal $\eta$} (derive with 2nd-order Taylor exp. of NLL around $\vw^{(k)}$):\\
$\eta(k)=\frac{||\nabla\mathrm{NLL}(\vw^{(k)})||^2}{\nabla\mathrm{NLL}(\vw^{(k)})^TH_{\mathrm{NLL}}(\vw^{(k)})\nabla\mathrm{NLL}(\vw^{(k)})}$

\subsection*{Newton's Method}
\mbox{$\vw^{(k+1)}=\vw^{(k)} - H_{\mathrm{NLL}}^{-1}(\vw^{(k)})\nabla\mathrm{NLL}(w^{(k)})$}\\
Derive with 2nd-order Taylor exp. of NLL around $\vw^{(k)}$