\section*{Ensemble Methods}
\subsection*{Bagging}
Given dataset $Z ={(x_i,y_i)}$, ind. (\emph{not possible!}) bootstrap samples $(Z'_i) \sim Z$, classifiers $b, b^{(i)}$ trained on $Z, (Z'_i)$, bagging creates classifier $\bar{b}^{(M)}(x) = \frac{1}{M}\sum b^{(i)}(x)$. We have $\E[(Y-\bar{b}^{(M)}(X))^2] \leq \E[(Y-b(X))^2]$.\\
\textbf{Random forests:} Train ensemble with bagging, randomly subsample features for each splitting step.\\
\textbf{Randomized Feature Selection:} Create datasets $X_i = \bar{T_i}^TX\bar{S_i}$ with $T_i \overset{\mathrm{P= \eta}}{\subset} [n]$, $S_i \overset{\mathrm{P=\eta}}{\subset} [p]$ and $\bar{A} =$\\
\emph{Theorems for case of linear regression:}
\emph{\textbf{Opt.:}} $\inf_{\alpha\leq\frac{p}{n}}\lim_{M,n,p\to\infty}\E[R(\beta^{(M)})] = \inf_{\lambda}\lim_{M,n,p\to\infty}\E[R(\hat{\beta}_{\lambda}^{\mathrm{ridge}})]$ and $\inf_{\lambda}\hat{\beta}_{\lambda}^{\mathrm{ridge}}$ is asympt. opt., where $\beta^{(i)} = \argmin_\beta ||X_i\bar{S}_t^T\beta - \bar{T}_i^T\vy||^2_2$ s.t. $(S^c_t)^T\beta=0$.\\
\mbox{\emph{\textbf{Regu.:}} If $X_i = X_i\bar{S_i}$ then $\beta^{(M)}\overset{p}{\to}\alpha\hat{\beta}^{\mathrm{LS}}$.}
\emph{\textbf{SGD+Dropout = RFS+Ensemble:}} If $\beta^{(t+t)} = \beta{(t)}-\eta(t)\nabla_\beta||X_i\bar{S_i}\bar{S_i}^T\beta - \vy||^2_2$ then $\beta^{(t)} \overset{p}{\to}\argmin_\beta \frac{1}{M}\sum_{t\leq M}R^{(t)}(\beta)$.

\subsection*{Boosting (> Forests > Bagging)}
\textbf{AdaBoost:}\\
Init: $b^{(0)}=0$, $\vw=\frac{1}{n}$, $b^{(i)}\in\{0,1\}$\\
\emph{Train, Eval, Add, Reweight, {\tiny\CircArrowRight{}}:}\\
$b^{(t)} = \argmin_b \mathcal{L}^{(\vw)}(b)=\sum w_i\mathbf{1}_{b(x_i)\neq y_i}$
$\epsilon_t=\mathcal{L}^{(\vw)}(b^{(t)})$\\
$b^{(t)} = b^{(t-1)} + \alpha_t b^{(t)}$, $\alpha_t = \log(\frac{1}{\mathrm{err}_t}-1)$.\\
$w_i \leftarrow w_i\exp(\alpha_t\mathbf{1}_{b^{t}(x_i) \neq y_i})$\\
\textbf{Viewpoints:}\\
\mbox{\emph{\textbf{FSAM:}} \emph{(= AdaBoost for $\mathcal{L}=\exp(-yy')$}}
$(\alpha_{t+1}, b^{(t+1)}) = \argmin\sum\mathcal{L}(y_i, \alpha_i b(x_i) + f_t(x_i))$.
\emph{\textbf{Maximum Margin:}} Assume wlog. $\sum \alpha_i = 1$. Then $\mathrm{margin}(x_i)=|\sum_{i:b^{(t)}(x_i)=1}\alpha_i - \sum_{i:b^{(t)}(x_i)=-1}\alpha_i| \to 1$
With $p\geq 1-\delta$ $\bar{b}^{(M)} \leq O(\frac{1}{n}\frac{1}{\mu^2}\log|\mathcal{H}|\ldots)$ + frac. of $x$ with $\mathrm{margin}(x) \leq \mu$. frac. of $x$ with $\mathrm{margin}(x) \leq \mu$ decays exp. with $M$.AdaBoost yields AdaBoost yields 
\emph{\textbf{spiky interpolation}} + \emph{\textbf{self-averaging}} (multiple models cancel out each others noise). \emph{\textbf{Double descent:}} \emph{Underparam. ($\gamma = \frac{p}{n}\in(0,1))$:} $\lim_{n,p\to\infty} R(\hat{\beta}^{\mathrm{OLS}}) = \sigma^2\frac{\gamma}{1-\gamma}$ \emph{Overparam. ($\gamma = \frac{p}{n}\in(1,\infty))$:} $\lim_{n,p\to\infty} R(\hat{\beta}^{\mathrm{OLS}}) = \sigma^2\frac{1}{\gamma-1}$ 

\section*{PAC learning}
\subsection*{Function of interest}
The probability of large excess error:\\
$\mathbb{P}[\hat{c}_N(X)\neq c^\text{Bayes}(X)|\mathcal{Z}]<\delta$.\\
But: could be unlucky with $\mathcal{Z}$, $c^\text{Bayes}$ not in hypoth. class, $\mathbb{P}[\dots]$ is a r.v., not scalar quality measure. Strat: average.
$\mathbb{P}[\mathcal{R}(\hat{c})-\inf_{c\in\mathcal{C}}\mathcal{R}(c)>\epsilon]<\delta$. Where $\mathcal{R}[\hat{c}]$ is the generalization error of the trained class., should not exceed the min. generalization error achievable by more than $\epsilon$.
Leads to:\\
$\sum_{c\in\mathcal{C}}\mathbb{P}[\lvert \hat{\mathcal{R}}_n(c)-\mathcal{R}(c)\rvert>\epsilon]\leq 2Ne^{-2n\epsilon^2}$. Def RHS$=\delta$: $\epsilon=\sqrt{\frac{\log N - \log(\delta/2)}{2n}}$. For $N$ the size of hypothesis class, and for $n$ the num. of samples. The expected error of $c$ thus depends on $1/\sqrt{n}$ and $\log N$!
\subsection*{Rectangle learning}
Pick tight rectangle. Diff. between picked rectangle $\hat{R}$ and true $R$ with few examples. Rectangles are \textbf{efficiently PAC learnable}: runs in polynom. $1/\epsilon$ (error param.) and $1/\delta$ (confidence val.).
\subsection*{Hyperplane learning}
Hypothesis: $\sum_{i=1}^d a_ix_i + a_0$ (all possible hyperplanes through $d$-dim vector) has \#-of-possible-classifiers $2\binom{n}{d}$. In class: the classifiers $c$ and $\hat{c}$ differ for no more than $d$ data points on a plane, IF found with ERM: $\forall_{c\in\mathcal{C}} \hat{\mathcal{R}}_n(c) \geq \hat{\mathcal{R}}_n(\hat{c}) - \frac{d}{n}$.
\subsection*{VC dimension}
How effectively a classifier (e.g., interval, unions of intervals, circles) can select subsets. Classifying with a closed iterval: $V_C=2$, for you cannot select the begin-point and end-point but not middle point! For unions, $V_c=2k$, for unit circles $V_c=3$, for finite-dimensional vector space $V_c\leq\dim(\mathcal{G})$.
\subsection*{From practical}
$\mathcal{R}_n(\hat{c})-\inf_{c\in\mathcal{C}}\mathcal{R}(c) \leq 2\sup_{c\in\mathcal{C}}\lvert \hat{\mathcal{R}}_n(c) - \mathcal{R}(c) \rvert$ for any class $\mathcal{C}$. Finite class:
$\mathbb{P}[2\sup_{c\in\mathcal{C}}\lvert \hat{\mathcal{R}}_n(c) - \mathcal{R}(c) \rvert>\epsilon]\leq 2\lvert\mathcal{C}\rvert e^{-2n\epsilon^2}$.
Shattering $s(\mathcal{A},n)$ is the score... For half-planes $s(\mathcal{A},2)=4$, $s(\mathcal{A},3)=8$, \dots. For triangles $s(\mathcal{A},2)=4,s(\mathcal{A},3)=8,s(\mathcal{A},4)=16$, powers.

\section*{Nonparametric Bayesian methods}
\textbf{Conjugate prior:} For parametric fam. $\mathcal{F}$, prior $p(\theta)\in\mathcal{F}$ is conj. for lik. $p(x|\theta)$ if $p(\theta|x)\in\mathcal{F}$.
\emph{Example (\textbf{NIW}):} Prior $\mathbf{\mu},\mathbf{\Sigma}\sim\mathrm{NIW}(m_0,K_0,v_0,S_0)$, Likelihood
$p(\mX|\mathbf{\mu},\mathbf{\Sigma}) = \prod_i \mathcal{N}(\rvx_i|\mathbf{\mu},\mathbf{\Sigma})$.
Posterior $\mathbf{\mu},\mathbf{\Sigma}|X\sim\mathrm{NIW}(m_p,K_p,v_p,S_p)$.\\
\mbox{\textbf{Semi-conjugate prior:} $\mathbf{\mu}\sim\mathcal{N}(m_0,V_0)$,}\\$\mathbf{\Sigma}\sim\mathrm{IV}(v0,S_0^{-1})$ is semi-conj. for lik. $p(\mX|\mathbf{\mu},\mathbf{\Sigma}) = \prod_i \mathcal{N}(\rvx_i|\mathbf{\mu},\mathbf{\Sigma})$ coz $\mathbf{\mu}|\mathbf{\Sigma},\mX\sim\mathcal{N}(m_p,V_p)$ and $\mathbf{\Sigma}|\mathbf{\mu},\mX\sim\mathrm{IV}(v_p,S_p^{-1})$.\\
\textbf{Gibbs sampling:} Given $\Theta^{(0)} = [\theta^{(0)}_0\ldots\theta^{(0)}_k]$, sample $\theta_j^{(t+1)}\sim p(\cdot|\Theta_{-j})$.\\
\textbf{\emph{Collapsed} Gibbs sampling:} marginalize out $\theta_i$: $\theta_j^{(t+1)}\sim p(\cdot|\Theta_{-i,j})$ \emph{Reduces variance --- Rao-Blackwellization}\\
\textbf{MCMC:} Sample from $p(\theta)$ using MC $\pi$ satisfying
(1) irreducibility, (2) aperiodicity, (3) $\pi(\theta'|\theta)p(\theta) = \pi(\theta|\theta')p(\theta)$.

\textbf{d-separation:} \emph{Given:} Observed RVs $Z$.
RVs $A$, $B$ are d-separated along path $p$ if p contains
any pattern of {\color{red}add}:\\\\

$A \bot B | Z$ if $A$, $B$ are d-separated along every path.

\subsection*{Nonparametric GMMs}
\emph{Three equivalent formalizations:}\\
\textbf{GEM:} Semi-conj. prior $\forall k\in\mathbb{N}.\,\mu_k,\Sigma_k\sim\mathrm{NIW}(\ldots)$, $\pi\sim\mathrm{GEM}(\alpha > 0)$, $\forall i\leq N.\,z_i\sim\pi$.
GEM defined by stick-breaking process $\beta_i\sim B(1,\alpha)$, $\pi_i\sim\beta_i\prod_{j<i}(1-\beta_j)$.\\
\textbf{CRP:} $\rvz\sim\mathrm{CRP}(\alpha)$, $P(z_{n+1}=k|z_{1:n})= \frac{\alpha}{\alpha+n-1} {\color{black}\textrm{ if }} k=(\max z_i+1) {\color{black}\mathrm{ else }} \frac{\#\{j:z_j=k\}}{\alpha+n-1}$. Expected \# of clusters: $O(\alpha\log(n)$. $(z_i)$'s exchangeable! 
\textbf{Dirichlet Process:} $f\sim\mathrm{DP}(\mathrm{NIW}(\ldots),\alpha) {\color{black}\implies} $
$f(\mu,\Sigma)=\sum_{k\geq1}\pi_k\mathbf{1}_{(\mu_k,\Sigma_k)=(\mu,\Sigma)}$ where $\pi\sim\mathrm{GEM}(\alpha)$ and $(\mu_k,\Sigma_k)\sim\mathrm{NIW}(\ldots)$.