\section*{Ensemble Methods}
\subsection*{Bagging}
Given dataset $Z ={(x_i,y_i)}$, ind. (\emph{not possible!}) bootstrap samples $(Z'_i) \sim Z$, classifiers $b, b^{(i)}$ trained on $Z, (Z'_i)$, bagging creates classifier $\bar{b}^{(M)}(x) = \frac{1}{M}\sum b^{(i)}(x)$. We have $\E[(Y-\bar{b}^{(M)}(X))^2] \leq \E[(Y-b(X))^2]$.\\
\textbf{Random forests:} Train ensemble with bagging, randomly subsample features for each splitting step.\\
\textbf{Randomized Feature Selection:} Create datasets $X_i = \bar{T_i}^TX\bar{S_i}$ with $T_i \overset{\mathrm{P= \eta}}{\subset} [n]$, $S_i \overset{\mathrm{P=\eta}}{\subset} [p]$ and $\bar{A} =$\\
\emph{Theorems for case of linear regression:}
\emph{\textbf{Opt.:}} $\inf_{\alpha\leq\frac{p}{n}}\lim_{M,n,p\to\infty}\E[R(\beta^{(M)})] = \inf_{\lambda}\lim_{M,n,p\to\infty}\E[R(\hat{\beta}_{\lambda}^{\mathrm{ridge}})]$ and $\inf_{\lambda}\hat{\beta}_{\lambda}^{\mathrm{ridge}}$ is asympt. opt., where $\beta^{(i)} = \argmin_\beta ||X_i\bar{S}_t^T\beta - \bar{T}_i^T\vy||^2_2$ s.t. $(S^c_t)^T\beta=0$.\\
\mbox{\emph{\textbf{Regu.:}} If $X_i = X_i\bar{S_i}$ then $\beta^{(M)}\overset{p}{\to}\alpha\hat{\beta}^{\mathrm{LS}}$.}
\emph{\textbf{SGD+Dropout = RFS+Ensemble:}} If $\beta^{(t+t)} = \beta{(t)}-\eta(t)\nabla_\beta||X_i\bar{S_i}\bar{S_i}^T\beta - \vy||^2_2$ then $\beta^{(t)} \overset{p}{\to}\argmin_\beta \frac{1}{M}\sum_{t\leq M}R^{(t)}(\beta)$.

\subsection*{Boosting (> Forests > Bagging)}
\textbf{AdaBoost:}\\
Init: $b^{(0)}=0$, $\vw=\frac{1}{n}$, $b^{(i)}\in\{0,1\}$\\
\emph{Train, Eval, Add, Reweight, {\tiny\CircArrowRight{}}:}\\
$b^{(t)} = \argmin_b \mathcal{L}^{(\vw)}(b)=\sum w_i\mathbf{1}_{b(x_i)\neq y_i}$
$\epsilon_t=\mathcal{L}^{(\vw)}(b^{(t)})$\\
$b^{(t)} = b^{(t-1)} + \alpha_t b^{(t)}$, $\alpha_t = \log(\frac{1}{\mathrm{err}_t}-1)$.\\
$w_i \leftarrow w_i\exp(\alpha_t\mathbf{1}_{b^{t}(x_i) \neq y_i})$\\
\textbf{Viewpoints:}\\
\mbox{\emph{\textbf{FSAM:}} \emph{(= AdaBoost for $\mathcal{L}=\exp(-yy')$}}
$(\alpha_{t+1}, b^{(t+1)}) = \argmin\sum\mathcal{L}(y_i, \alpha_i b(x_i) + f_t(x_i))$.
\emph{\textbf{Maximum Margin:}} Assume wlog. $\sum \alpha_i = 1$. Then $\mathrm{margin}(x_i)=|\sum_{i:b^{(t)}(x_i)=1}\alpha_i - \sum_{i:b^{(t)}(x_i)=-1}\alpha_i| \to 1$
With $p\geq 1-\delta$ $\bar{b}^{(M)} \leq O(\frac{1}{n}\frac{1}{\mu^2}\log|\mathcal{H}|\ldots)$ + frac. of $x$ with $\mathrm{margin}(x) \leq \mu$. frac. of $x$ with $\mathrm{margin}(x) \leq \mu$ decays exp. with $M$.AdaBoost yields AdaBoost yields 
\emph{\textbf{spiky interpolation}} + \emph{\textbf{self-averaging}} (multiple models cancel out each others noise). \emph{\textbf{Double descent:}} \emph{Underparam. ($\gamma = \frac{p}{n}\in(0,1))$:} $\lim_{n,p\to\infty} R(\hat{\beta}^{\mathrm{OLS}}) = \sigma^2\frac{\gamma}{1-\gamma}$ \emph{Overparam. ($\gamma = \frac{p}{n}\in(1,\infty))$:} $\lim_{n,p\to\infty} R(\hat{\beta}^{\mathrm{OLS}}) = \sigma^2\frac{1}{\gamma-1}$ 

\section*{PAC learning}
\subsection*{Function of interest}
The probability of large excess error:\\
$\mathbb{P}[\hat{c}_N(X)\neq c^\text{Bayes}(X)|\mathcal{Z}]<\delta$.\\
But: could be unlucky with $\mathcal{Z}$, $c^\text{Bayes}$ not in hypoth. class, $\mathbb{P}[\dots]$ is a r.v., not scalar quality measure. Strat: average.
$\mathbb{P}[\mathcal{R}(\hat{c})-\inf_{c\in\mathcal{C}}\mathcal{R}(c)>\epsilon]<\delta$. Where $\mathcal{R}[\hat{c}]$ is the generalization error of the trained class., should not exceed the min. generalization error achievable by more than $\epsilon$.
Leads to:\\
$\sum_{c\in\mathcal{C}}\mathbb{P}[\lvert \hat{\mathcal{R}}_n(c)-\mathcal{R}(c)\rvert>\epsilon]\leq 2Ne^{-2n\epsilon^2}$. Def RHS$=\delta$: $\epsilon=\sqrt{\frac{\log N - \log(\delta/2)}{2n}}$. For $N$ the size of hypothesis class, and for $n$ the num. of samples. The expected error of $c$ thus depends on $1/\sqrt{n}$ and $\log N$!
\subsection*{Rectangle learning}
Pick tight rectangle. Diff. between picked rectangle $\hat{R}$ and true $R$ with few examples. Rectangles are \textbf{efficiently PAC learnable}: runs in polynom. $1/\epsilon$ (error param.) and $1/\delta$ (confidence val.).
\subsection*{Hyperplane learning}
Hypothesis: $\sum_{i=1}^d a_ix_i + a_0$ (all possible hyperplanes through $d$-dim vector) has \#-of-possible-classifiers $2\binom{n}{d}$. In class: the classifiers $c$ and $\hat{c}$ differ for no more than $d$ data points on a plane, IF found with ERM: $\forall_{c\in\mathcal{C}} \hat{\mathcal{R}}_n(c) \geq \hat{\mathcal{R}}_n(\hat{c}) - \frac{d}{n}$.
\subsection*{VC dimension}
How effectively a classifier (e.g., interval, unions of intervals, circles) can select subsets. Classifying with a closed iterval: $V_C=2$, for you cannot select the begin-point and end-point but not middle point! For unions, $V_c=2k$, for unit circles $V_c=3$, for finite-dimensional vector space $V_c\leq\dim(\mathcal{G})$.
\subsection*{From practical}
$\mathcal{R}_n(\hat{c})-\inf_{c\in\mathcal{C}}\mathcal{R}(c) \leq 2\sup_{c\in\mathcal{C}}\lvert \hat{\mathcal{R}}_n(c) - \mathcal{R}(c) \rvert$ for any class $\mathcal{C}$. Finite class:
$\mathbb{P}[2\sup_{c\in\mathcal{C}}\lvert \hat{\mathcal{R}}_n(c) - \mathcal{R}(c) \rvert>\epsilon]\leq 2\lvert\mathcal{C}\rvert e^{-2n\epsilon^2}$.
Shattering $s(\mathcal{A},n)$ is the score... For half-planes $s(\mathcal{A},2)=4$, $s(\mathcal{A},3)=8$, \dots. For triangles $s(\mathcal{A},2)=4,s(\mathcal{A},3)=8,s(\mathcal{A},4)=16$, powers.

\section*{Nonparametric Bayesian methods}
$\text{Beta}(x|a,b)=B(a,b)^{-1} x^{a-1}(1-x)^{b-1}$: prob. of Bernoulli proc. after observing $a-1$ success and $b-1$ failures. Expended to multivariate case with Dirichlet distr. That will give multivar. probs, \textit{based on finite} counts! But we don't know exactly which multivar. distribution works. With more data, we update the Dirichlet distribution. Is a conjugate prior.\\
\textbf{Stick-breaking Dirichl. proc.}. Repeatedly draw from $\text{Beta}(x|1,\alpha)$ with fixed $\alpha$, but from reducing stick: $\rho_k=\beta_k(1-\sum_{i=1}^{k-1}\rho_i)$. The prior:\\
$\mathbb{P}[z_i=k|z_{-i},\alpha]=\begin{cases}\frac{N_{k,-i}}{\alpha+N-1} & \text{exist. }K \\ \frac{\alpha}{\alpha+N-1} & \text{oth.}\end{cases}$. Final Gibbs sampler:\\
$\mathbb{P}[z_i=k|z_{-i},\alpha,\mu]=\begin{cases}\frac{N_{k,-i}}{\alpha+N-1}p(x_i|x_{-i,k},\mu) & \text{exist. }K \\ \frac{\alpha}{\alpha+N-1}p(x_i,\mu) & \text{oth.}\end{cases}$

\subsection*{Gibbs sampling}:\\
Init: assign all data to a cluster, with prior $\pi_i$, with $\sum_{k=1}^K\pi_i<1$ (s.t. new clusters possible). E.g. with stick-breaking. Then remove $x$ from $k$ and compute new $\theta_k$, then compute Gibbs sampler prob. (CRP), and sample the new cluster assignment $z_i\sim p(z_i|x_{-i},\theta_k)$. If cluster is empty, remove it and decrease $K$.