\section*{Ensemble Methods}
\subsection*{Bagging}
Given dataset $Z =\{(x_i,y_i)\}\sim D$, ind. (\emph{not possible!}) bootstrap samples $(Z'_i) \sim Z$, classifiers $b$, $b^{(i)}$ trained on $Z$, $(Z'_i)$, bagging creates classifier $\bar{b}^{(M)}(x) = \frac{1}{M}\sum b^{(i)}(x)$. We have $\E_{\color{gray}D,Y,X}[(Y-\bar{b}^{(M)}(X))^2] \leq \E[(Y-b(X))^2]$.\\
\textbf{Random forests:} Train ensemble with bagging, randomly subsample features for each splitting step.\\
\textbf{Randomized Feature Selection:} For $X\in\mathbb{R}^{n\times p}$, create datasets \mbox{$X_i = \bar{T_i}^TX\bar{S_i}$ with $T_i \overset{\mathrm{P= \eta}}{\subset} [n]$, $S_i \overset{\mathrm{P=\alpha}}{\subset} [p]$.}\\
\emph{For linear regression:}
\emph{Optimality Th.:} $\inf_{\alpha<\frac{p}{n}}\lim_{M,n,p\to\infty}\E_{X,S_i,T_i}[R(\bar{\beta}^{(M)})] = \inf_{\lambda}\lim_{M,n,p\to\infty}\E_{X,S_i,T_i}[R(\hat{\beta}_{\lambda}^{\mathrm{ridge}})]$  where $\beta^{(i)} = \argmin_\beta ||X_i\bar{S}_t^T\beta - \bar{T}_i^T\vy||^2_2$ s.t. $(S^c_t)^T\beta=0$.\\ (Fact: $\inf_{\lambda}\hat{\beta}_{\lambda}^{\mathrm{ridge}}$ is the asympt. opt. linear regression estimator)
\emph{Regularization Th:} If $\beta^{(i)}=\argmin_{\beta:({S_i^c}^T\beta=0)}||X\beta-Y||^2$ then $\bar{\beta}^{(M)}\overset{p}{\to}\alpha\hat{\beta}^{\mathrm{LS}}$.\\
\emph{\textbf{SGD+Dropout = RFS+Ensemble:}} \mbox{If $\beta^{(t+1)}\!=\!\beta{(t)}-\eta(t)\nabla_\beta||X_i\bar{S_i}\bar{S_i}^T\beta - \vy||^2_2$} then $\beta^{(t)}\!
\!\overset{p}{\to}\argmin_\beta \frac{1}{M}\sum_{t\leq M}R^{(t)}(\beta)\overset{p}{\to}\alpha^{-1}\beta^{\mathrm{ridge}}_{\frac{1-\alpha}{\alpha}}$.

\subsection*{Boosting (> Forests > Bagging)}
\textbf{AdaBoost:}\\
Init: $\tilde{b}^{(0)}=0$, $\vw=\frac{1}{n}$\\
\emph{Train, Eval, Add, Reweight, {\tiny\CircArrowRight{}}:}\\
0. Define $\mathcal{L}^{(\vw)}(b)=\sum w_i\mathbf{1}_{b(x_i)\neq y_i}$\\
1. $b^{(t)} = \argmin_b \mathcal{L}^{(\vw)}(b) = \mathrm{err_t}$\\
\mbox{2. $\tilde{b}^{(t)}\!=\!\tilde{b}^{(t-1)}\!+\!\alpha_t b^{(t)}$, $\alpha_t\!=\!\log(\frac{1}{\mathrm{err}_t}-1)$}\\
3. $w_i \leftarrow w_i\exp(\alpha_t\mathbf{1}\{b^{t}(x_i) \neq y_i\})$\\
4. $\vw \leftarrow \vw / ||\vw||$.\\
\textbf{Viewpoints:}\\
\mbox{\emph{\textbf{FSAM:}} \emph{(= AdaBoost for $\mathcal{L}=\exp(-yy')$}}
$(\alpha_{t+1}, b^{(t+1)}) = \argmin\sum\mathcal{L}(y_i, \alpha_i b(x_i) + f_t(x_i))$.
\emph{\textbf{Maximum Margin:}} Assume wlog. $\sum \alpha_i = 1$. Then $\mathrm{margin}(x_i)=|\sum_{i:b^{(t)}(x_i)=1}\alpha_i - \sum_{i:b^{(t)}(x_i)=-1}\alpha_i| \to 1$
With $p\geq 1-\delta$ $\mathrm{gen\_error}(\bar{b}^{(M)}) \leq O(\frac{1}{n}\frac{1}{\mu^2}\log|\mathcal{H}|\ldots) + \textrm{frac. of x with margin}(x) \leq \mu$ and frac. of $x$ with $\mathrm{margin}(x) \leq \mu$ decays exp. with $M$. AdaBoost yields 
\emph{\textbf{spiky interpolation}} + \emph{\textbf{self-averaging}} (multiple models cancel out each others noise). \emph{\textbf{Double descent:}} \emph{Underparam. ($\gamma = \frac{p}{n}\in(0,1))$:} $\lim_{n,p\to\infty} R(\hat{\beta}^{\mathrm{OLS}}) = \sigma^2\frac{\gamma}{1-\gamma}$ \emph{Overparam. ($\gamma = \frac{p}{n}\in(1,\infty))$:} $\lim_{n,p\to\infty} R(\hat{\beta}^{\mathrm{OLS}}) = (1-\frac{1}{\gamma})+\sigma^2\frac{1}{\gamma-1}$ 

\section*{PAC learning}
Consider risk $\mathcal{R}(c) = P(c(X)\neq Y)$.
\textbf{Strong PAC learnability:}
For instance space $\mathcal{X}$, algorithm $\mathcal{A}$ can learn concept class
$\mathcal{C}\subset\mathcal{P}(\mathcal{X})$ from hypothesis class $\mathcal{H}\subset\mathcal{P}(\mathcal{X})$ if $\exists\mathrm{poly}(\cdot,\cdot,\cdot)$ s.t.
$\forall$dists $\mathcal{D}$ on $\mathcal{X}\times\{0,1\}$, $\forall 0<\epsilon<\frac{1}{2}$, $0<\delta<\frac{1}{2}$, $\forall Z=\{(X_i,Y_i)\}\sim\mathcal{D}$ with $n\geq\mathrm{poly}(\frac{1}{\epsilon},\frac{1}{\delta},\mathrm{size}(c))$, $\hat{c} = \mathcal{A}(Z)\in\mathcal{H}$ satisfies $P_Z(\mathcal{R}(\hat{c})-\inf_{c\in\mathcal{C}}\mathcal{R}(c)\leq\epsilon) \geq 1-\delta$.
\textbf{Efficient PAC learnability:} $\mathcal{A}$ runs in $O(\mathrm{poly}(\frac{1}{\delta},\frac{1}{\epsilon}))$.\\
\textbf{Weak PAC learnability:} Allow ``larger'' $\epsilon$, e.g. only req. $\epsilon < \frac{1}{2}$.

\textbf{VC-dimension:} $A\subset\mathcal{X}$ can be \textbf{shattered} by $\mathcal{C}$ if $\forall S\subset A.\, \exists c\in\mathcal{C}.\, c\cap A = S$.
\textbf{VC}($\mathcal{C}$) = max $n$ s.t. $\exists A\subset\mathcal{X}$ with $|A|=n$ and $\mathcal{C}$ shatters $A$. 

\textbf{VC-Theorem:} Let $c^* = \argmin_{c\in\mathcal{C}}(c)$. Then, $\mathcal{R}(\hat{c}^*_n) - \mathcal{R}(c^*)  = \mathcal{R}(\hat{c}^*_n) - \mathcal{R}_n(\hat{c}^*_n) + \mathcal{R}_n(\hat{c}^*_n) - \mathcal{R}(c^*) \leq 2\sup_c|\mathcal{R}(c)-\mathcal{R}_n(c)|$.

\textbf{Bayes error:} $\inf_{f\in\mathcal{X}\to\{0,1\}}\mathcal{R}(f)$. For 0/1-loss: $f^*(x) = \argmax_{y\in\{0,1\}}p(y|x)$.

\subsection*{PAC theorems \normalfont \emph{(Assume $\mathcal{H} = \mathcal{C}$.)}}
\textbf{(1) Finite-$|\mathcal{C}|$:} For \textbf{consistent hypothesis} $\hat{c}=\mathcal{A}(Z)$, i.e. $\forall Z.\,\mathcal{\hat{R}}(\hat{c},Z)=0$, if $n\geq \frac{1}{\epsilon}(\log\mathcal{H}+\log\frac{1}{\delta})$ then $P(\mathcal{R}(\hat{c}) \leq \epsilon) \geq 1-\delta$.\\
\textbf{(2) Finite-$|\mathcal{C}|$:} Let $\hat{c}^*_n = \mathrm{ERM}(Z) = \argmin_{c\in\mathcal{C}}\frac{1}{n}\sum_{i\leq n}\mathbf{1}\{c(X_i)\neq Y_i\}$. Then, $P(\mathcal{R}(\hat{c}^*_n) - \inf_{c\in\mathcal{C}}\mathcal{R}(c) > \epsilon) \leq 2|\mathcal{C}|\exp(-2n\epsilon^2)$.\\
\textbf{(3) VC:} (If $\mathrm{VC}_\mathcal{C}>2$?) then 
$P(\mathcal{R}(\hat{c}^*_n) - \inf_{c\in\mathcal{C}}\mathcal{R}(c) > \epsilon) \leq 9n^{\mathrm{VC}_\mathcal{C}}\exp(-\frac{n\epsilon^2}{32})$.\\
\textbf{Hyperplane learning:} Reduce inf. set of hypotheses $\sum_{i\leq d}(a_i x_i + a_0)$ to $\binom{n}{d}$ hyperplanes defined by $P\subset Z$ points with $|P| = d$. \emph{Theorem:} If $n\geq d$, $2\frac{d}{n}\leq\epsilon$, then $P(\mathcal{R}(\hat{c}^*_n)-\inf_{c\in\mathcal{C}}\mathcal{R}(c)>\epsilon) \leq \epsilon^{2d\epsilon}(2\binom{n}{d}+1)\exp(-\frac{n\epsilon^2}{2})$.

\section*{Nonparametric Bayesian methods}
\textbf{Conjugate prior:} For parametric fam. $\mathcal{F}$, prior $p(\theta)\in\mathcal{F}$ is conj. for lik. $p(x|\theta)$ if $p(\theta|x)\in\mathcal{F}$.
\emph{Example (\textbf{NIW}):} Prior $\mathbf{\mu},\mathbf{\Sigma}\sim\mathrm{NIW}(m_0,K_0,v_0,S_0)$, Likelihood
$p(\mX|\mathbf{\mu},\mathbf{\Sigma}) = \prod_i \mathcal{N}(\rvx_i|\mathbf{\mu},\mathbf{\Sigma})$.
Posterior $\mathbf{\mu},\mathbf{\Sigma}|X\sim\mathrm{NIW}(m_p,K_p,v_p,S_p)$.\\
\mbox{\textbf{Semi-conjugate prior:} $\mathbf{\mu}\sim\mathcal{N}(m_0,V_0)$,}\\$\mathbf{\Sigma}\sim\mathrm{IV}(v_0,S_0^{-1})$ is semi-conj. for lik. $p(\mX|\mathbf{\mu},\mathbf{\Sigma}) = \prod_i \mathcal{N}(\rvx_i|\mathbf{\mu},\mathbf{\Sigma})$ coz $\mathbf{\mu}|\mathbf{\Sigma},\mX\sim\mathcal{N}(m_p,V_p)$ and $\mathbf{\Sigma}|\mathbf{\mu},\mX\sim\mathrm{IV}(v_p,S_p^{-1})$.\\
\textbf{Gibbs sampling:} Given $\Theta^{(0)} = [\theta^{(0)}_0\ldots\theta^{(0)}_k]$, sample $\theta_j^{(t+1)}\sim p(\cdot|\theta_0^{t+1},\ldots,\theta_{j-1}^{t+1},\theta_{j+1}^t,\ldots,\theta_k^t)$.\\
\textbf{\emph{Collapsed} Gibbs sampling:} marginalize out $\theta_i$: $\theta_j^{(t+1)}\sim p(\cdot|\Theta_{-i,j})$ \emph{Reduces variance --- Rao-Blackwellization}\\
\textbf{MCMC:} Sample from $p(\theta)$ using MC $\pi$ satisfying
(1) irreducibility, (2) aperiodicity, (3) $\pi(\theta'|\theta)p(\theta) = \pi(\theta|\theta')p(\theta')$.

\textbf{d-separation:} \emph{Given:} Observed RVs $Z$.
RVs $A$, $B$ are d-separated along path $p$ if p contains
any pattern of $\circ>\bullet>\circ$, $\circ<\bullet>\circ$, $\circ>\Delta<\circ$.\\
\mbox{$A \bot B | Z$ if $\forall$paths $A$, $B$ d-separated.}

\subsection*{Nonparametric GMMs}
\emph{Three equivalent formalizations:}\\
\textbf{GEM:} Semi-conj. prior $\forall k\in\mathbb{N}.\,\mu_k,\Sigma_k\sim\mathrm{NIW}(\ldots)$, $\pi\sim\mathrm{GEM}(\alpha > 0)$, $\forall i\leq N.\,z_i\sim\pi$.
GEM defined by stick-breaking process $\beta_i\sim B(1,\alpha)$\\$\propto(1-x)^{\alpha-1}$, $\pi_i=\beta_i\prod_{j<i}(1-\beta_j)$.\\
\textbf{CRP:} $\rvz\sim\mathrm{CRP}(\alpha)$, $P(z_{n+1}=k|z_{1:n})= \frac{\alpha}{\alpha+n-1} {\color{black}\textrm{ if }} k=(\max z_i+1) {\color{black}\mathrm{ else }} \frac{\#\{j:z_j=k\}}{\alpha+n-1}$. Expected \# of clusters: $O(\alpha\log(n)$. $(z_i)$'s exchangeable! 
\textbf{Dirichlet Process:} $f\sim\mathrm{DP}(\mathrm{NIW}(\ldots),\alpha) {\color{black}\implies} $
$f(\mu,\Sigma)=\sum_{k\geq1}\pi_k\mathbf{1}_{(\mu_k,\Sigma_k)=(\mu,\Sigma)}$ with $\pi\sim\mathrm{GEM}(\alpha)$ \& $(\mu_k,\Sigma_k)\sim\mathrm{NIW}(\ldots)$.